{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cf25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments,TrainerCallback, EarlyStoppingCallback\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63cae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "DATASET_NAME = \"stanfordnlp/imdb\"\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED) # if using CPU\n",
    "torch.cuda.manual_seed(SEED) # if using single-GPU\n",
    "torch.cuda.manual_seed_all(SEED) # if using multi-GPU\n",
    "torch.backends.cudnn.deterministic = True # deterministic mode\n",
    "torch.backends.cudnn.benchmark = False # disable auto-tuner to find the best algorithm to use for your hardware\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow TensorFloat-32 on matmul operations\n",
    "torch.backends.cudnn.allow_tf32  = True # allow TensorFloat-32 on convolution operations\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2b61c",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbaf423",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564191dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just take the first 50 tokens for speed/running on cpu\n",
    "def truncate(example):\n",
    "    return {\n",
    "        'text': \" \".join(example['text'].split()[:50]),\n",
    "        'label': example['label']\n",
    "    }\n",
    "\n",
    "small_dataset = DatasetDict(\n",
    "    train=dataset['train'].shuffle(seed=SEED).select(range(128)).map(truncate),\n",
    "    val=dataset['train'].shuffle(seed=SEED).select(range(128, 160)).map(truncate),\n",
    ")\n",
    "print(small_dataset)\n",
    "print(small_dataset['train'][:10])\n",
    "print(f\"Train size: {len(small_dataset['train'])}\")\n",
    "print(f\"Val size: {len(small_dataset['val'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dc1047",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2509402",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee756a",
   "metadata": {},
   "source": [
    "# Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tokenized_dataset = small_dataset.map(\n",
    "    lambda example: tokenizer(example['text'], padding=True, truncation=True), # https://huggingface.co/docs/transformers/pad_truncation\n",
    "    batched=True,\n",
    "    batch_size=16\n",
    ")\n",
    "small_tokenized_dataset = small_tokenized_dataset.remove_columns([\"text\"])\n",
    "small_tokenized_dataset = small_tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "small_tokenized_dataset.set_format(\"torch\")\n",
    "print(small_tokenized_dataset['train'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(small_tokenized_dataset['train'], batch_size=16, shuffle=True)\n",
    "valloader = DataLoader(small_tokenized_dataset['val'], batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f03a5",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc1fd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "model = model_init() # Create one instance for summary\n",
    "summary(model, input_size=(16, 50), col_names=('input_size', 'output_size', 'num_params', 'trainable'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac2e352",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": np.mean(predictions == labels),\n",
    "        \"f1\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 2e-5, 5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0.0, 0.01, 0.1]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 3),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32]),\n",
    "    }\n",
    "\n",
    "arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"epoch\", # run validation at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    seed=SEED\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=arguments,\n",
    "    train_dataset=small_tokenized_dataset['train'],\n",
    "    eval_dataset=small_tokenized_dataset['val'], # change to test when you do your final evaluation!\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    # will call on_log on each logging step, specified by TrainerArguement. (i.e TrainerArguement.logginng_step)\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "trainer.add_callback(LoggingCallback(\"./results/log.jsonl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cba68d",
   "metadata": {},
   "source": [
    "# Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\", \n",
    "    backend=\"optuna\", \n",
    "    hp_space=hp_space, \n",
    "    n_trials=5,\n",
    "    compute_objective=lambda metrics: metrics['eval_accuracy']\n",
    ")\n",
    "# Update trainer with best run hyperparameters and train final model\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb35098",
   "metadata": {},
   "source": [
    "# Exectution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c2e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just gets evaluation metrics\n",
    "# results = trainer.evaluate()\n",
    "# also gives you predictions\n",
    "results = trainer.predict(small_tokenized_dataset['val'])\n",
    "# Report metrics and inference time\n",
    "print(\"--- Evaluation Results ---\")\n",
    "print(f\"Accuracy: {results.metrics['test_accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {results.metrics['test_f1']:.4f}\")\n",
    "print(f\"Inference Time: {results.metrics['test_runtime']:.4f} seconds\")\n",
    "print(f\"Inference Speed: {results.metrics['test_samples_per_second']:.2f} samples/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e0020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load our saved model, we can pass the path to the checkpoint into the `from_pretrained` method:\n",
    "test_str = \"I enjoyed the movie!\"\n",
    "finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"./results/checkpoint-???\")\n",
    "model_inputs = tokenizer(test_str, return_tensors=\"pt\")\n",
    "prediction = torch.argmax(finetuned_model(**model_inputs).logits)\n",
    "print([\"NEGATIVE\", \"POSITIVE\"][prediction])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
