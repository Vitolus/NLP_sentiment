{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d07995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "from collections import deque\n",
    "import cv2\n",
    "import lmdb\n",
    "import pickle\n",
    "import json\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchinfo import summary\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from transformers import Trainer, TrainingArguments,TrainerCallback, EarlyStoppingCallback\n",
    "from datasets import load_dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb94a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "DATASET_NAME = \"stanfordnlp/imdb\"\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED) # if using CPU\n",
    "torch.cuda.manual_seed(SEED) # if using single-GPU\n",
    "torch.cuda.manual_seed_all(SEED) # if using multi-GPU\n",
    "torch.backends.cudnn.deterministic = True # deterministic mode\n",
    "torch.backends.cudnn.benchmark = False # disable auto-tuner to find the best algorithm to use for your hardware\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow TensorFloat-32 on matmul operations\n",
    "torch.backends.cudnn.allow_tf32  = True # allow TensorFloat-32 on convolution operations\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print(\"Using device: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01934c",
   "metadata": {},
   "source": [
    "# Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3f09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "print(dataset)\n",
    "#%% Prompt Strategies\n",
    "def truncate_few_shot(example):\n",
    "    # Few-shot prompt design: providing examples to the LLM\n",
    "    review_segment = \" \".join(example['text'].split()[:50])\n",
    "    prompt = (\n",
    "        \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\n\"\n",
    "        \"Review: The movie was terrible, boring and too long.\\n\"\n",
    "        \"Sentiment: NEGATIVE\\n\\n\"\n",
    "        \"Review: Absolutely fantastic! I loved every minute of it.\\n\"\n",
    "        \"Sentiment: POSITIVE\\n\\n\"\n",
    "        f\"Review: {review_segment}\\n\"\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    return {'text': prompt, 'label': example['label']}\n",
    "\n",
    "def truncate_zero_shot(example):\n",
    "    # Zero-shot prompt design: No examples, just instruction\n",
    "    review_segment = \" \".join(example['text'].split()[:50])\n",
    "    prompt = (\n",
    "        \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\n\"\n",
    "        f\"Review: {review_segment}\\n\"\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    return {'text': prompt, 'label': example['label']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2ef394",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48c031",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# Ensure pad token is set for Phi-3 (often missing in LLMs)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token if tokenizer.unk_token else tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "print(tokenizer)\n",
    "#%% Model Definition\n",
    "def model_init():\n",
    "    # Load Phi-3 for sequence classification\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, \n",
    "        num_labels=2, \n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # Freeze the backbone to perform Linear Probing\n",
    "    # This keeps the LLM knowledge intact and only trains the classification head\n",
    "    # This is crucial to fit training on standard GPUs without LoRA\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model\n",
    "\n",
    "model = model_init() # Create one instance for summary\n",
    "try:\n",
    "    # Adjusted input size for summary to match prompt length roughly\n",
    "    summary(model, input_size=(1, 128), col_names=('input_size', 'output_size', 'num_params', 'trainable'), dtypes=[torch.IntTensor])\n",
    "except Exception as e:\n",
    "    print(f\"Summary skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1046ad99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    \"\"\"Called at the end of validation. Gives accuracy\"\"\"\n",
    "    logits, labels = pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": np.mean(predictions == labels),\n",
    "        \"f1\": f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        # Higher LR for head tuning\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-3, log=True),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0.0, 0.01]),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 3),\n",
    "        # Smaller batch sizes for LLM memory constraints\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [1, 2, 4]),\n",
    "    }\n",
    "\n",
    "class LoggingCallback(TrainerCallback):\n",
    "    def __init__(self, log_path):\n",
    "        self.log_path = log_path\n",
    "    # will call on_log on each logging step, specified by TrainerArguement. (i.e TrainerArguement.logginng_step)\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n",
    "            with open(self.log_path, \"a\") as f:\n",
    "                f.write(json.dumps(logs) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ad1d1",
   "metadata": {},
   "source": [
    "# Experiment Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5337326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(name, map_fn):\n",
    "    print(f\"\\n\\n{'='*20} Running Experiment: {name} {'='*20}\")\n",
    "    # Prepare Data\n",
    "    current_dataset = DatasetDict(\n",
    "        train=dataset['train'].shuffle(seed=SEED).select(range(128)).map(map_fn),\n",
    "        val=dataset['train'].shuffle(seed=SEED).select(range(128, 160)).map(map_fn),\n",
    "    )\n",
    "    tokenized_dataset = current_dataset.map(\n",
    "        lambda example: tokenizer(example['text'], padding=True, truncation=True),\n",
    "        batched=True,\n",
    "        batch_size=16\n",
    "    )\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "    tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    # Setup Trainer\n",
    "    arguments = TrainingArguments(\n",
    "        output_dir=f\"./results/{name}\",\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-4,\n",
    "        load_best_model_at_end=True,\n",
    "        seed=SEED,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_dir=f\"./results/{name}/logs\"\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model_init=model_init,\n",
    "        args=arguments,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['val'],\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=1, early_stopping_threshold=0.0))\n",
    "    trainer.add_callback(LoggingCallback(f\"./results/{name}/log.jsonl\"))\n",
    "    # Hyperparameter Search\n",
    "    print(f\"--- Tuning Hyperparameters for {name} ---\")\n",
    "    best_run = trainer.hyperparameter_search(\n",
    "        direction=\"maximize\", \n",
    "        backend=\"optuna\", \n",
    "        hp_space=hp_space, \n",
    "        n_trials=5,\n",
    "        compute_objective=lambda metrics: metrics['eval_accuracy']\n",
    "    )\n",
    "    # Train Best Model\n",
    "    print(f\"--- Training Best Model for {name} ---\")\n",
    "    for n, v in best_run.hyperparameters.items():\n",
    "        setattr(trainer.args, n, v)\n",
    "    trainer.train()\n",
    "    # Evaluate\n",
    "    results = trainer.predict(tokenized_dataset['val'])\n",
    "    # Cleanup to free VRAM for next run\n",
    "    del trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    return results.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd8d1e",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff68d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Few-Shot Training\n",
    "fs_metrics = run_experiment(\"Few_Shot\", truncate_few_shot)\n",
    "# Run Zero-Shot Training\n",
    "zs_metrics = run_experiment(\"Zero_Shot\", truncate_zero_shot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dd4442",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c08ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n=== FINAL COMPARISON ===\")\n",
    "print(f\"{'Metric':<20} | {'Few-Shot':<15} | {'Zero-Shot':<15}\")\n",
    "print(\"-\" * 56)\n",
    "print(f\"{'Accuracy':<20} | {fs_metrics['test_accuracy']:.4f}          | {zs_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"{'F1 Score':<20} | {fs_metrics['test_f1']:.4f}          | {zs_metrics['test_f1']:.4f}\")\n",
    "print(f\"{'Inference Time (s)':<20} | {fs_metrics['test_runtime']:.4f}          | {zs_metrics['test_runtime']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyatom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
