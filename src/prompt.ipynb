{
 "cells": [
  {
   "cell_type": "code",
   "id": "14d07995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:37.218868400Z",
     "start_time": "2026-02-11T15:03:37.141981500Z"
    }
   },
   "source": [
    "import gc\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "from transformers import Trainer, TrainingArguments,TrainerCallback, EarlyStoppingCallback\n",
    "from transformers import DataCollatorWithPadding\n",
    "from datasets import load_dataset, DatasetDict"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "ddb94a94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:37.482791500Z",
     "start_time": "2026-02-11T15:03:37.218868400Z"
    }
   },
   "source": [
    "SEED = 42\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "DATASET_NAME = \"stanfordnlp/imdb\"\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED) # if using CPU\n",
    "torch.cuda.manual_seed(SEED) # if using single-GPU\n",
    "torch.cuda.manual_seed_all(SEED) # if using multi-GPU\n",
    "torch.backends.cudnn.deterministic = True # deterministic mode\n",
    "torch.backends.cudnn.benchmark = False # disable auto-tuner to find the best algorithm to use for your hardware\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow TensorFloat-32 on matmul operations\n",
    "torch.backends.cudnn.allow_tf32  = True # allow TensorFloat-32 on convolution operations\n",
    "# torch.autograd.set_detect_anomaly(True) # keep this commented out for speed unless debugging NaN\n",
    "print(\"Using device: \", DEVICE)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset loading",
   "id": "3b928cac27395b9a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:39.144111100Z",
     "start_time": "2026-02-11T15:03:37.482791500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "print(dataset)"
   ],
   "id": "f28a7e85baabdf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    unsupervised: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:39.266141300Z",
     "start_time": "2026-02-11T15:03:39.198151300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def truncate_few_shot(example):\n",
    "    # Few-shot prompt design: providing examples to the LLM\n",
    "    # Take the first 128 words of the review for better context\n",
    "    review_segment = \" \".join(example['text'].split()[:128])\n",
    "    prompt = (\n",
    "        \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\n\"\n",
    "        \"Review: The movie was terrible, boring and too long.\\n\"\n",
    "        \"Sentiment: NEGATIVE\\n\\n\"\n",
    "        \"Review: Absolutely fantastic! I loved every minute of it.\\n\"\n",
    "        \"Sentiment: POSITIVE\\n\\n\"\n",
    "        f\"Review: {review_segment}\\n\"\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    return {'text': prompt, 'label': example['label']}\n",
    "\n",
    "small_test_dataset = dataset['test'].shuffle(seed=SEED).select(range(250))\n",
    "print(small_test_dataset)\n",
    "\n",
    "small_few_shot = small_test_dataset.map(truncate_few_shot)\n",
    "print(small_few_shot)\n",
    "print(small_few_shot[:2])\n",
    "print(f\"Test size: {len(small_few_shot)}\")"
   ],
   "id": "e45d530804308cb4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 250\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 250\n",
      "})\n",
      "{'text': [\"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\nReview: The movie was terrible, boring and too long.\\nSentiment: NEGATIVE\\n\\nReview: Absolutely fantastic! I loved every minute of it.\\nSentiment: POSITIVE\\n\\nReview: <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly\\nSentiment:\", \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\nReview: The movie was terrible, boring and too long.\\nSentiment: NEGATIVE\\n\\nReview: Absolutely fantastic! I loved every minute of it.\\nSentiment: POSITIVE\\n\\nReview: This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit\\nSentiment:\"], 'label': [1, 1]}\n",
      "Test size: 250\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:40.771510600Z",
     "start_time": "2026-02-11T15:03:39.268157300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def truncate_zero_shot(example):\n",
    "    # Zero-shot prompt design: No examples, just instruction\n",
    "    review_segment = \" \".join(example['text'].split()[:128])\n",
    "    prompt = (\n",
    "        \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\n\"\n",
    "        f\"Review: {review_segment}\\n\"\n",
    "        \"Sentiment:\"\n",
    "    )\n",
    "    return {'text': prompt, 'label': example['label']}\n",
    "\n",
    "small_zero_shot = small_test_dataset.map(truncate_zero_shot)\n",
    "print(small_zero_shot)\n",
    "print(small_zero_shot[:2])\n",
    "print(f\"Test size: {len(small_zero_shot)}\")"
   ],
   "id": "4067396201afabd1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 250\n",
      "})\n",
      "{'text': [\"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\nReview: <br /><br />When I unsuspectedly rented A Thousand Acres, I thought I was in for an entertaining King Lear story and of course Michelle Pfeiffer was in it, so what could go wrong?<br /><br />Very quickly, however, I realized that this story was about A Thousand Other Things besides just Acres. I started crying and couldn't stop until long after the movie ended. Thank you Jane, Laura and Jocelyn, for bringing us such a wonderfully subtle and compassionate movie! Thank you cast, for being involved and portraying the characters with such depth and gentleness!<br /><br />I recognized the Angry sister; the Runaway sister and the sister in Denial. I recognized the Abusive Husband and why he was there and then the Father, oh oh the Father... all superbly\\nSentiment:\", \"You are a sentiment classifier. Determine if the following movie reviews are POSITIVE or NEGATIVE.\\n\\nReview: This is the latest entry in the long series of films with the French agent, O.S.S. 117 (the French answer to James Bond). The series was launched in the early 1950's, and spawned at least eight films (none of which was ever released in the U.S.). 'O.S.S.117:Cairo,Nest Of Spies' is a breezy little comedy that should not...repeat NOT, be taken too seriously. Our protagonist finds himself in the middle of a spy chase in Egypt (with Morroco doing stand in for Egypt) to find out about a long lost friend. What follows is the standard James Bond/Inspector Cloussou kind of antics. Although our man is something of an overt xenophobe,sexist,homophobe, it's treated as pure farce (as I said, don't take it too seriously). Although there is a bit\\nSentiment:\"], 'label': [1, 1]}\n",
      "Test size: 250\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenizer",
   "id": "b936225b7feb670d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:42.479493500Z",
     "start_time": "2026-02-11T15:03:40.829784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# Set padding token if not set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # LLMs usually need left padding for generation\n",
    "print(tokenizer)"
   ],
   "id": "afca7b985e3efb0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenizersBackend(name_or_path='microsoft/Phi-3-mini-4k-instruct', vocab_size=32000, model_max_length=4096, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '<|endoftext|>', 'unk_token': '<unk>', 'pad_token': '<|endoftext|>'}, added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t32000: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32001: AddedToken(\"<|assistant|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32002: AddedToken(\"<|placeholder1|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32003: AddedToken(\"<|placeholder2|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32004: AddedToken(\"<|placeholder3|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32005: AddedToken(\"<|placeholder4|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32006: AddedToken(\"<|system|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32007: AddedToken(\"<|end|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32008: AddedToken(\"<|placeholder5|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32009: AddedToken(\"<|placeholder6|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32010: AddedToken(\"<|user|>\", rstrip=True, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset Preprocessing",
   "id": "fc58c3603ffcf999"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:03:42.629155300Z",
     "start_time": "2026-02-11T15:03:42.535572100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "small_few_shot_tokenized = small_few_shot.map(tokenize_fn, batched=True, batch_size=16)\n",
    "small_few_shot_tokenized = small_few_shot_tokenized.rename_column(\"label\", \"labels\")\n",
    "small_few_shot_tokenized = small_few_shot_tokenized.remove_columns([\"text\"])\n",
    "small_few_shot_tokenized.set_format(\"torch\")\n",
    "\n",
    "small_zero_shot_tokenized = small_zero_shot.map(tokenize_fn, batched=True, batch_size=16)\n",
    "small_zero_shot_tokenized = small_zero_shot_tokenized.rename_column(\"label\", \"labels\")\n",
    "small_zero_shot_tokenized = small_zero_shot_tokenized.remove_columns([\"text\"])\n",
    "small_zero_shot_tokenized.set_format(\"torch\")\n",
    "\n",
    "print(small_few_shot_tokenized[0:2])"
   ],
   "id": "bb166924c1722bdf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([1, 1]), 'input_ids': [tensor([  887,   526,   263, 19688,   770,  3709, 29889,  5953,   837,   457,\n",
      "          565,   278,  1494, 14064, 21804,   526,   349,  3267,  1806, 18474,\n",
      "          470,   405, 11787,  1299, 18474, 29889,    13,    13,  1123,  1493,\n",
      "        29901,   450, 14064,   471, 16403, 29892,   289,  8253,   322,  2086,\n",
      "         1472, 29889,    13, 29903,   296,  2073, 29901,   405, 11787,  1299,\n",
      "        18474,    13,    13,  1123,  1493, 29901,  1976,  2929, 11579, 13568,\n",
      "         6288, 29991,   306, 18012,  1432, 11015,   310,   372, 29889,    13,\n",
      "        29903,   296,  2073, 29901,   349,  3267,  1806, 18474,    13,    13,\n",
      "         1123,  1493, 29901,   529,  1182,  2900, 29966,  1182,  2900, 10401,\n",
      "          306,  9644,   375,  6021,   368,   364, 14927,   319,   498,   681,\n",
      "          392,  7255,   690, 29892,   306,  2714,   306,   471,   297,   363,\n",
      "          385, 22684,   292,  4088, 19530,  5828,   322,   310,  3236,  3375,\n",
      "         1808,   349,  1725,  8349,   471,   297,   372, 29892,   577,   825,\n",
      "         1033,   748,  2743, 29973, 29966,  1182,  2900, 29966,  1182,  2900,\n",
      "        29963,   708,  9098, 29892,  3138, 29892,   306, 16387,   393,   445,\n",
      "         5828,   471,  1048,   319,   498,   681,   392,  5901, 28706, 18034,\n",
      "          925,  7255,   690, 29889,   306,  4687, 10901,   292,   322,  8496,\n",
      "        29915, 29873,  5040,  2745,  1472,  1156,   278, 14064,  9698, 29889,\n",
      "         3374,   366, 10447, 29892, 21671,   322,   435,   542,   873, 29876,\n",
      "        29892,   363, 20794,   502,  1316,   263,  4997,  3730, 12059,   280,\n",
      "          322,   752,   465,   291,   403, 14064, 29991,  3374,   366,  4320,\n",
      "        29892,   363,  1641,  9701,   322,  2011,   764,   292,   278,  4890,\n",
      "          411,  1316, 10809,   322,  8116,  2435,   404, 29991, 29966,  1182,\n",
      "         2900, 29966,  1182,  2900, 29902, 14831,   278,   530, 14793,  9883,\n",
      "        29936,   278,  7525, 21694,  9883,   322,   278,  9883,   297,  3384,\n",
      "          616, 29889,   306, 14831,   278,  1976,   375,   573, 24287,  4980,\n",
      "          322,  2020,   540,   471,   727,   322,   769,   278, 17852, 29892,\n",
      "         9360,  9360,   278, 17852,   856,   599,  2428, 29890,   368,    13,\n",
      "        29903,   296,  2073, 29901]), tensor([  887,   526,   263, 19688,   770,  3709, 29889,  5953,   837,   457,\n",
      "          565,   278,  1494, 14064, 21804,   526,   349,  3267,  1806, 18474,\n",
      "          470,   405, 11787,  1299, 18474, 29889,    13,    13,  1123,  1493,\n",
      "        29901,   450, 14064,   471, 16403, 29892,   289,  8253,   322,  2086,\n",
      "         1472, 29889,    13, 29903,   296,  2073, 29901,   405, 11787,  1299,\n",
      "        18474,    13,    13,  1123,  1493, 29901,  1976,  2929, 11579, 13568,\n",
      "         6288, 29991,   306, 18012,  1432, 11015,   310,   372, 29889,    13,\n",
      "        29903,   296,  2073, 29901,   349,  3267,  1806, 18474,    13,    13,\n",
      "         1123,  1493, 29901,   910,   338,   278,  9281,  6251,   297,   278,\n",
      "         1472,  3652,   310, 12298,   411,   278,  5176, 10823, 29892,   438,\n",
      "        29889, 29903, 29889, 29903, 29889, 29871, 29896, 29896, 29955,   313,\n",
      "         1552,  5176,  1234,   304,  5011, 26370,   467,   450,  3652,   471,\n",
      "        15241,   297,   278,  4688, 29871, 29896, 29929, 29945, 29900, 29915,\n",
      "        29879, 29892,   322, 29178,   287,   472,  3203,  9475, 12298,   313,\n",
      "         9290,   310,   607,   471,  3926,  5492,   297,   278,   501, 29889,\n",
      "        29903,  6250,   525, 29949, 29889, 29903, 29889, 29903, 29889, 29896,\n",
      "        29896, 29955, 29901, 29907, 26025, 29892, 29940,   342,  4587,  1706,\n",
      "          583, 29915,   338,   263,   289,   929,  1537,  2217, 26228,   393,\n",
      "          881,   451,   856, 14358,  6058, 29892,   367,  4586,  2086, 25798,\n",
      "        29889,  8680, 15572,   391, 14061,  3654,   297,   278,  7256,   310,\n",
      "          263,   805, 29891,   521,   559,   297, 12892,   313,  2541,  3879,\n",
      "          307,  1111,  2599,  2317,   297,   363, 12892, 29897,   304,  1284,\n",
      "          714,  1048,   263,  1472,  5714,  5121, 29889,  1724,  4477,   338,\n",
      "          278,  3918,  5011, 26370, 29914,   797, 21494,   272,  2233, 21167,\n",
      "          283,  2924,   310,  3677,  1199, 29889,  8512,  1749,   767,   338,\n",
      "         1554,   310,   385,   975, 29873,   921,   264,  3021, 16945, 29892,\n",
      "        14167,   391, 29892,  9706,  3021, 16945, 29892,   372, 29915, 29879,\n",
      "        14914,   408,  8296,  2215,   346,   313,   294,   306,  1497, 29892,\n",
      "         1016, 29915, 29873,  2125,   372,  2086, 25798,   467,  8512,   727,\n",
      "          338,   263,  2586,    13, 29903,   296,  2073, 29901])], 'attention_mask': [tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Definition",
   "id": "d6b7935174ebbf9f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:05:00.899634400Z",
     "start_time": "2026-02-11T15:03:42.630165700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# IMPORTANT: Do not set rope_scaling to None on Phi-3 configs with transformers>=5.\n",
    "# Doing so will set rope_parameters=None internally and crash the native Phi3 implementation.\n",
    "# Leave the default as-is; if needed, adjust rope_type explicitly (e.g., to 'linear').\n",
    "# Example of safe adjustment (commented out):\n",
    "# if isinstance(config.rope_scaling, dict) and config.rope_scaling.get(\"rope_type\") == \"default\":\n",
    "#     config.rope_scaling[\"rope_type\"] = \"linear\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    "    trust_remote_code=False,\n",
    "    torch_dtype=\"auto\"\n",
    ").to(DEVICE)\n",
    "\n",
    "def get_sentiment_batch(batch_input_ids, batch_attention_mask):\n",
    "    # batch_input_ids and batch_attention_mask are already padded tensors\n",
    "    inputs = {\n",
    "        'input_ids': batch_input_ids.to(DEVICE),\n",
    "        'attention_mask': batch_attention_mask.to(DEVICE)\n",
    "    }\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "    with torch.no_grad():\n",
    "        # return_dict_in_generate=True lets us easily separate input from output\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=5,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False\n",
    "        )\n",
    "    # Extract only the generated tokens\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "    batch_preds = []\n",
    "    for decoded in decoded_preds:\n",
    "        prediction_part = decoded.strip().upper()\n",
    "        if \"POSITIVE\" in prediction_part:\n",
    "            batch_preds.append(1)\n",
    "        elif \"NEGATIVE\" in prediction_part:\n",
    "            batch_preds.append(0)\n",
    "        else:\n",
    "            # Fallback\n",
    "            batch_preds.append(0)\n",
    "    return batch_preds\n",
    "\n",
    "def evaluate_prompting(dataset_to_eval, batch_size=16):\n",
    "    preds = []\n",
    "    labels = []\n",
    "    # DataCollatorWithPadding handles padding within batches\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    # Create a DataLoader for batching\n",
    "    dataloader = DataLoader(dataset_to_eval, batch_size=batch_size, collate_fn=data_collator, pin_memory=True)\n",
    "    # Synchronize CUDA so timing reflects only the generation loop (like Trainer.predict)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    start_time = time.perf_counter()\n",
    "    for batch in tqdm(dataloader):\n",
    "        batch_input_ids = batch['input_ids']\n",
    "        batch_attention_mask = batch['attention_mask']\n",
    "        batch_preds = get_sentiment_batch(batch_input_ids, batch_attention_mask)\n",
    "        preds.extend(batch_preds)\n",
    "        labels.extend(batch['labels'].tolist())\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    samples_per_second = len(dataset_to_eval) / duration if duration > 0 else 0\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "    return {\n",
    "        \"accuracy\": np.mean(preds == labels),\n",
    "        \"f1\": f1_score(labels, preds, average='weighted'),\n",
    "        \"runtime\": duration,\n",
    "        \"samples_per_second\": samples_per_second\n",
    "    }"
   ],
   "id": "6ad025d963dcff0f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/195 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c220c2cc679640b4afc3204c3818fa74"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Execution",
   "id": "4c0ebe6aa0af4e45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:05:22.290465700Z",
     "start_time": "2026-02-11T15:05:00.954205100Z"
    }
   },
   "cell_type": "code",
   "source": "results_few_shot = evaluate_prompting(small_few_shot_tokenized)",
   "id": "446bac73fed38c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e49522ffb529452a8c2495b6ae515c09"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:05:36.006166800Z",
     "start_time": "2026-02-11T15:05:22.340310100Z"
    }
   },
   "cell_type": "code",
   "source": "results_zero_shot = evaluate_prompting(small_zero_shot_tokenized)",
   "id": "8a3f011ab484dc74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "efa50b3762be4bd389a7c6469dea0598"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Comparison\n",
    "\n",
    "Note: Prompting using Phi-3 (3.8B parameters) is significantly more computationally expensive than classic fine-tuning with DistilBERT (~66M parameters) because it involves autoregressive generation of multiple tokens instead of a single forward pass for classification."
   ],
   "id": "ae3c37a744f3a606"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-11T15:05:36.132339400Z",
     "start_time": "2026-02-11T15:05:36.059902300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"--- FINAL COMPARISON ---\")\n",
    "print(f\"Model used: {MODEL_NAME}\")\n",
    "print(f\"{'Metric':<30} | {'Few-Shot':<15} | {'Zero-Shot':<15}\")\n",
    "print(\"-\" * 66)\n",
    "print(f\"{'Accuracy':<30} | {results_few_shot['accuracy']:.4f}  | {results_zero_shot['accuracy']:.4f}\")\n",
    "print(f\"{'F1 Score':<30} | {results_few_shot['f1']:.4f}  | {results_zero_shot['f1']:.4f}\")\n",
    "print(f\"{'Inference Time (s)':<30} | {results_few_shot['runtime']:.4f}  | {results_zero_shot['runtime']:.4f}\")\n",
    "print(f\"{'Inference Speed (samples/s)':<30} | {results_few_shot['samples_per_second']:.4f}  | {results_zero_shot['samples_per_second']:.4f}\")"
   ],
   "id": "406f6994f9369119",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FINAL COMPARISON ---\n",
      "Model used: microsoft/Phi-3-mini-4k-instruct\n",
      "Metric                         | Few-Shot        | Zero-Shot      \n",
      "------------------------------------------------------------------\n",
      "Accuracy                       | 0.8960  | 0.7800\n",
      "F1 Score                       | 0.8958  | 0.7679\n",
      "Inference Time (s)             | 19.1458  | 12.2918\n",
      "Inference Speed (samples/s)    | 13.0577  | 20.3388\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyatom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
